{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a888c666-aa07-4fd6-9b04-d8b9b5a0cecb",
   "metadata": {},
   "source": [
    "<span>\n",
    "<img align=\"right\" width=\"320\" src=\"http://www.sobigdata.eu/sites/default/files/SoBigData_RI_3.png\">\n",
    "</span>\n",
    "<span>\n",
    "<b>Author:</b> <a href=\"http://about.giuliorossetti.net\">Giulio Rossetti and Katherine Abramski</a><br/>\n",
    "<b>Python version:</b>  3.9<br/>\n",
    "<b>Last update:</b> 19/09/2023\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fba038-37fc-40e8-91b4-62b044f22c7b",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "# *Mixing R and Python for Statistical Analyses*\n",
    "This notebook contains an introduction to rpy2, a Python package that allows a basilar integration of R and Python, as well as some examples that demonstrate how to approach some statistical analyses.\n",
    "\n",
    "**Note:** this notebook is purposely not 100% comprehensive, it only discusses the basic things you need to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be2913-2543-4016-a5ec-ccc85717c697",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Integrating R and Python](#R)\n",
    "    1. [Push: Passing Variables from Python to R](#i)\n",
    "    2. [Pull: Passing Variables from  R to Python](#o)\n",
    "2. [Statistics Fundamentals](#rev)\n",
    "    1. [The Normal Distribution](#normal)\n",
    "    2. [Hypothesis Testing](#hyp)\n",
    "3. [Statistical Tests](#tests)\n",
    "    1. [Is there a relationship between part of speech (noun/verb) and gender?](#t_test)\n",
    "    2. [Are polysemous words more concrete than non-polysemous words?](#mw_test)\n",
    "    3. [Is there a relationship between part of speech (noun, verb, adjective) and polysemy?](#Chi)\n",
    "    4. [Can imageability be used to predict word concreteness?](#reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65abd1d-530a-4a11-8dc3-d2578fc12298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the python libraries\n",
    "\n",
    "from functools import partial\n",
    "from rpy2.ipython import html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b51f7d-8c6b-4222-9c1e-db60007f873b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the %%R magic \n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36649c1-7334-4ac5-bb81-09dc23844054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Install R packages that we will use\n",
    "# You only need to do this once\n",
    "\n",
    "install.packages(\"tidyverse\")\n",
    "install.packages(\"stats\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"corrplot\")\n",
    "install.packages(\"MASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baaa59-bdae-4a6c-b75f-c4a149843c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load R packages that we will use\n",
    "library(tidyverse)\n",
    "library(stats)\n",
    "library(ggplot2)\n",
    "library(corrplot)\n",
    "library(MASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528a974-0ef7-41ca-8f56-ebcc1954fd60",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='R'></a>\n",
    "## 1. Integrating R and Python ([to top](#top))\n",
    "\n",
    "To execute an R script in jupyter notebook, it is mandatory to specify that the cell does not contain Python code.\n",
    "\n",
    "Thic can be done by using \"%%R\" in the first line. Everything that follows should use the R syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826304f-d7a4-4a19-983f-83d8a8f8ad67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R # This specifies that the following code is R code, not python code\n",
    "\n",
    "x <- \"This is R code!\"\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c540686-d362-4c03-b3d0-7f471378d5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = \"This is python code!\"\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c74724-7826-4ecd-9870-3f857cf1d824",
   "metadata": {},
   "source": [
    "Once a notebook is designed to contain both Python and R cells it is possible to move computation results across the two interpreters using variable push/pull strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72fad8-94e7-4527-92f0-e9bd6db35689",
   "metadata": {},
   "source": [
    "<a id='i'></a>\n",
    "### 1.A Push: Passing Variables from Python to R ([to top](#top))\n",
    "\n",
    "Let us assume we have some data in python, stored in a dataframe, and we want to move it to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e8eea-3409-4dfb-b43a-0c8583c12791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a simple dataframe\n",
    "\n",
    "python_df = pd.DataFrame({'A': [1, 2, 3],\n",
    "                          'B': [4, 5, 6]},\n",
    "                          )  \n",
    "python_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd8fa6-ec2b-43fd-94b2-15c7fccbd5e3",
   "metadata": {},
   "source": [
    "Mapping the dataframe to the R environment (where it will be stored as an R dataframe) is called a \"push\" and can be achieved using \"-i\" followed by the name of the variable to push in the %%R magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcb21c-fc91-474d-81b2-321a76e391f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R -i python_df # pass the dataframe to R using \"-i\" followed by the variable name\n",
    "\n",
    "python_df # this a a R dataframe now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f77e7-2981-4e10-b4a3-f3b9d57b2028",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='o'></a>\n",
    "### 1.B Pull: Passing Variables from  R to Python ([to top](#top))\n",
    "\n",
    "Now let's assume we want to manipulate the dataframe in some way, and then send it back to R.\n",
    "\n",
    "Moving an object from R to Python is called a \"pull\" and can be achieved using \"-o\" follwed by the name of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2d065-e483-410e-a91d-8213be4c013e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R -o python_df2\n",
    "# python_df variable is still in R\n",
    "# let's add a new column to the dataframe (the sum of A and B), then push it to back python\n",
    "\n",
    "python_df2 <- python_df %>% mutate(C = A + B)\n",
    "python_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb4ded-710c-4366-b8e7-675e66d377af",
   "metadata": {},
   "source": [
    "Now the dataframe is back in python. Objects from the R environment are mapped either in a pandas dataframe (if possible), or in dictionaries (for complex objects like model outputs that cannot be mapped in a pandas dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96079d-3d88-4f3b-b701-4a0035233077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "python_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612022d-2c75-4d5e-8684-c838ec40edbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='rev'></a>\n",
    "## 2. Statistics Fundamentals ([to top](#top))\n",
    "\n",
    "Now let's review some basic statistical fundamentals: The Normal Distrubution, and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90907ad3-8326-4df5-a956-6863ca4824d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='normal'></a>\n",
    "### 2.A The Normal Distribution ([to top](#top))\n",
    "\n",
    "Let's start by looking at the distributions of men's and women's heights in Italy (using R).\n",
    "\n",
    "We know men's and women's heights in Italy are approximately **normally distributed** such that:\n",
    "- Men have a mean height of **175cm** with a standard deviation of **6cm**\n",
    "- Women have a mean height of **165cm** with a standard deviation of **5cm**\n",
    "\n",
    "These values reflect the population **parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85495ae-91d5-46b4-acdb-922e79abe8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "# Sample size\n",
    "n <- 1000\n",
    "\n",
    "# Height of Men\n",
    "M_mean <- 175\n",
    "M_SD <- 7\n",
    "M <- rnorm(n, M_mean, M_SD)\n",
    "\n",
    "# Height of Women\n",
    "W_mean <- 165\n",
    "W_SD <- 6\n",
    "W <- rnorm(n, W_mean, W_SD)\n",
    "\n",
    "print(round(head(M),1))\n",
    "print(round(head(W),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc42cfb-1f19-4b31-8a9d-08a7fbbd3c9e",
   "metadata": {},
   "source": [
    "Now from the samples let's:\n",
    "- calculate the **sample means** and **sample standard deviations**\n",
    "- view the **empirical** distributions of the sampled heights\n",
    "\n",
    "What happens to the sample means and sample standard deviations as we increase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e070e-36c4-41c4-83ab-e4a1c5ee4328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Calculate means and SDs\n",
    "print(paste0(\"M: sample mean = \", round(mean(M), 3), \", sample SD = \", round(sd(M), 3)))\n",
    "print(paste0(\"W: sample mean = \", round(mean(W), 3), \", sample SD = \", round(sd(W), 3)))\n",
    "\n",
    "# plot distributions\n",
    "par(mfrow = c(1,2))\n",
    "hist(M, breaks = 20, col = \"purple\")\n",
    "hist(W, breaks = 20, col = \"yellowgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e58b5c-021c-4496-8046-4b9aed1f93a5",
   "metadata": {},
   "source": [
    "#### What information can we get from a data distribution?\n",
    "A data distribution can give us important information about the probability of observing certain values.\n",
    "\n",
    "**What percentage of the sample of Italian women is at least 170cm tall?**\n",
    "\n",
    "We can get this number by simply counting the number of Italian women taller than 170cm in the sample and dividing by the total number in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db644a17-aa30-4a85-8972-9257703807e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "emp_p170 <- sum(W >= 170)/n * 100\n",
    "emp_p170"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48197363-7504-4197-b434-1d104f111a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "From our **empirical distribution**, we find that the probability that an Italian woman is at least 170cm tall is **19.7%**.\n",
    "\n",
    "But since we know the population **parameters** of Italian women's heights (mean = **165cm** and standard deviation = **6cm**) we can calculate the **probability that an Italian woman is at least 170cm** from the **theoretical distribution**.\n",
    "\n",
    "This is equivalent to the area under the curve of the probability distribution function (pdf) of the standard normal distribution with mean = 165 and SD = 5 to the right of 176cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d65a70-49dc-413d-949b-db53a48874b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "teo_p170 <- pnorm(170, W_mean, W_SD, lower.tail = FALSE) * 100\n",
    "print(teo_p170)\n",
    "# lower.tail = FALSE because we want the area to the RIGHT of 170\n",
    "# If we want the area to the LEFT of 170, we write lower.tail = TRUE\n",
    "\n",
    "# View the probability distribution function (pdf)\n",
    "x <- seq(from = W_mean - W_SD*4, to = W_mean + W_SD*4, by = 0.05)\n",
    "norm_dat <- data.frame(dist = \"N(1,4)\", x = x, pdf = dnorm(x, mean = W_mean, sd = W_SD))\n",
    "ggplot(norm_dat) + geom_line(aes(x = x, y = pdf)) + geom_vline(xintercept = 170, linetype = \"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90b012-08cc-4b8b-afea-263e6d7a118a",
   "metadata": {},
   "source": [
    "From our **theoretical distribution** we find that the probability that an Italian woman is at least 170cm is **20.2%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc5638-d0f7-4c7b-891c-b0d4dd96c78a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='hyp'></a>\n",
    "### 2.B Hypothesis Testing ([to top](#top))\n",
    "\n",
    "Hypothesis testing is used to make inferences about a population using sampled data. Usually this involves testing some hypothesis about the relationship between variables of the population.\n",
    "\n",
    "To get the intuition of how hypothesis testing works, let's try to make an inference about whether or not Katie is Italian based on her height.\n",
    "\n",
    "#### Is Katie Italian?\n",
    "\n",
    "**Observed data:** Katie is 176cm tall.\n",
    "\n",
    "**Null Hypothesis:** $H_{0}$: Katie is Italian\n",
    "\n",
    "**Alternative Hypothesis:** $H_{A}$: Katie in not Italian\n",
    "\n",
    "**What is the probability that Katie is 176cm tall, assuming she is Italian?**\n",
    "\n",
    "In other words...\n",
    "\n",
    "What is the **probability that an Italian woman is at least 176cm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d3884-3d09-4e43-895f-cbc8759e9c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "teo_p176 <- pnorm(176, W_mean, W_SD, lower.tail = FALSE) * 100\n",
    "print(teo_p176)\n",
    "# lower.tail = FALSE because we want the area to the RIGHT of 170\n",
    "# If we want the area to the LEFT of 170, we write lower.tail = TRUE\n",
    "\n",
    "# View the probability distribution function (pdf)\n",
    "x <- seq(from = W_mean - W_SD*4, to = W_mean + W_SD*4, by = 0.05)\n",
    "norm_dat <- data.frame(dist = \"N(1,4)\", x = x, pdf = dnorm(x, mean = W_mean, sd = W_SD))\n",
    "ggplot(norm_dat) + geom_line(aes(x = x, y = pdf)) + geom_vline(xintercept = 176, linetype = \"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a610025-a716-453e-9b59-07b1e3dd84cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "From our **theoretical distribution** reflecting the **null hypothesis** we find that the probability that an Italian woman is at least 176cm is **3.3%**.\n",
    "\n",
    "This value is the **p-value: the probability of observing the data assuming the null hypothesis is true.**\n",
    "\n",
    "Since the probability of observing the data (Katie is 176cm tall) assuming the null hypothesis is true (Katie is Italian) is very small (less than 5%) we say that we have sufficient evidence to **reject the null hypothesis.**\n",
    "\n",
    "Therefore, we infer that Katie is not Italian.\n",
    "\n",
    "In fact, she is American.\n",
    "\n",
    "#### How can hypothesis testing be applied to make inferences about populations from sampled data?\n",
    "\n",
    "The previous example was just to provide some intuition about what hypothesis testing is all about, specifically, how distributions can be used to calculate p-values to determine the significance of relationships. In the next part, we take a look at some examples of approaches to questions about relationships between variables using statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54665ec5-651a-4b95-9435-587dd934d797",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='tests'></a>\n",
    "## 3. Statistical Tests ([to top](#top))\n",
    "\n",
    "Statistical tests quantify the significance of **relationships between variables** of a population **inferred from sampled data**\n",
    "\n",
    "**IMPORTANT:** The type of statistical test that should be used depends mainly on:\n",
    "- the type of predictor and outcome variables (which help us determine our null and alternative hypotheses)\n",
    "- certain assumptions about the sampled data (which help us determine whether to use a parametric or non-parametric test)\n",
    "\n",
    "Let's have a look at choosing the right statistical test for finding relationships in the **Glasgow norms dataset**.\n",
    "\n",
    "Specifically we are interested in the following questions:\n",
    "\n",
    "1. [Is there a relationship between part of speech (noun/verb) and gender?](#t_test)\n",
    "2. [Are polysemous words more concrete than non-polysemous words?](#mw_test)\n",
    "3. [Is there a relationship between part of speech (noun/verb) and polysemy?](#Chi)\n",
    "5. [Can imageability be used to predict word concreteness?](#reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31007969-b3fe-43bc-afa8-d40d9d9b57b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset in python\n",
    "\n",
    "df = pd.read_csv(\"words_glasgow.csv\", skipinitialspace=True, sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67b17e-47e4-48f7-a312-eb5230a0a739",
   "metadata": {},
   "source": [
    "Let's add a variable for part of speech (noun, verb, other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ce77c-c277-4526-8bc0-19fe7d45efb1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add some parts of speech tags using nltk\n",
    "from nltk.tag import pos_tag\n",
    "pos_tags = [x[1] for x in pos_tag(list(df.word.values))]\n",
    "df['pos'] = pos_tags\n",
    "\n",
    "# Classify them as noun, verb, adjective, or other\n",
    "df['POS'] = 0\n",
    "df.loc[df['pos'].str.startswith('N'), 'POS'] = 1\n",
    "df.loc[df['pos'].str.startswith('V'), 'POS'] = 2\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9cdab-3027-4489-8865-120ea7b76961",
   "metadata": {},
   "source": [
    "Now let's send our dataframe to the R environment and have a look at the data before doing some statistical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b29061-7907-430c-9a94-db4ed20ff7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R -i df\n",
    "\n",
    "df_num <- df %>% dplyr::select(-word, -pos, -web_corpus_freq) %>% na.omit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ce9cd-7d1e-440f-a047-da4cab459bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# View correlation plot\n",
    "df_cor <- cor(df_num)\n",
    "corrplot(df_cor, type = \"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b586d2f-095d-4b0e-b2fe-5477c40d1067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# View histograms\n",
    "par(mfrow = c(3,4))\n",
    "for (i in 1:12) {\n",
    "  hist(df_num[,i], xlab = colnames(df_num)[i], main = \"\")\n",
    "}\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48b7b4-6eb8-47f4-94d3-57f68cbb0a50",
   "metadata": {},
   "source": [
    "<a id='t_test'></a>\n",
    "### 3.A Is there a relationship between part of speech (noun/verb) and gender? ([to top](#top))\n",
    "\n",
    "#### What test should we use?\n",
    "\n",
    "**Step 1: Understand type of predictor and outcome variables.**\n",
    "\n",
    "- Predictor(s): 1 **Categorical** predictor (part of speech) with 2 groups (noun, verb)\n",
    "- Outcome(s): 1 **Quantitative** outcome (gender)\n",
    "\n",
    "For this type of data, we should use either an **unpaired t-test** (parametric) or a **Mann-Whitney U test** (non-parametric). Whether we use a parametric test or a non-parametric test depends on the sample size (which we can control to some extent) and the assumptions of the sampled data. If the sample size is not too small, and the assumptions are met, we prefer a parametric test.\n",
    "\n",
    "**Step 2: Check the assumptions of the sampled data.**\n",
    "- The outcome data are approximately **normally distributed** for both groups \n",
    "- The outcome data have **similar variances** across groups\n",
    "\n",
    "Now let's get a **sample of our data**, have a look at it, and check our assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40383bd2-70e0-451d-809a-3f56e88880b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "n <- 100\n",
    "N_gen_samp <- sample_n(df %>% filter(POS == 1) %>% dplyr::select(word, POS, gender) %>% na.omit(), n)\n",
    "V_gen_samp <- sample_n(df %>% filter(POS == 2) %>% dplyr::select(word, POS, gender) %>% na.omit(), n)\n",
    "\n",
    "NV_gen_samp <- rbind(N_gen_samp, V_gen_samp)\n",
    "\n",
    "ggplot(NV_gen_samp, aes(x=POS %>% as.factor(), y=gender)) + geom_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1643938-41f3-4995-855d-8b8857617272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Are gender for nouns and gender for verbs approximately normally distributed? \n",
    "# test of normality: p-value <= 0.05 suggests data is NOT normally distributed\n",
    "print(shapiro.test(N_gen_samp$gender))\n",
    "print(shapiro.test(V_gen_samp$gender))\n",
    "\n",
    "# Do gender for nouns and gender for verbs have similar variances?\n",
    "# test of homogeneity of variances: p-value <= 0.05 suggests variances are equal\n",
    "print(var.test(gender ~ POS %>% as.factor(), data = NV_gen_samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec508372-65f7-401e-932d-afa9021ce500",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We should use the unpaired t-test\n",
    "Since both assumptions are satisfied and the sample size is not small, we choose the **unpaired t-test** (parametric test). Since we have chosed a parametric test, we are interested in a **difference in means**.\n",
    "\n",
    "Now we can define our null and alternative hypotheses:\n",
    "- $H_{0}$: Gender for nouns **does not differ from** gender for verbs\n",
    "- $H_{A}$: Gender for nouns **differs from** gender for verbs\n",
    "\n",
    "Since we do not have a specific hypothesis about the direction of the relationship, this is a **two-sided** test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9776d9-60cf-437a-87a1-725eede7c13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# t-test: p-value <= 0.05 suggests we should reject the null hypothesis\n",
    "T_Test <- t.test(gender ~ POS,\n",
    "                data = NV_gen_samp,\n",
    "                alternative = \"two.sided\",\n",
    "                paired = F)\n",
    "T_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abc578-f3ff-4851-a869-2703bbebb6f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Conclusion\n",
    "The p-value is greater than 0.05, which is our significance level, so we **fail to reject the null hypothesis**. Therefore we conclude that **there is no difference between the gender of nouns and verbs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be428545-6f23-4bdb-8b0a-e7e05b96b3ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='MWU_test'></a>\n",
    "### 3.B Are polysemous words more concrete than non-polysemous words? ([to top](#top))\n",
    "\n",
    "#### What test should we use?\n",
    "\n",
    "**Step 1: Understand type of predictor and outcome variables.**\n",
    "\n",
    "- Predictor(s): 1 **Categorical** predictor (polysemy) with 2 groups (polysemous, non-polysemous)\n",
    "- Outcome(s): 1 **Quantitative** outcome (concreteness)\n",
    "\n",
    "For this type of data, we should use either an **unpaired t-test** (parametric) or a **Mann-Whitney U test** (non-parametric).\n",
    "\n",
    "**Step 2: Check the assumptions of the sampled data.**\n",
    "- The outcome data are approximately **normally distributed** for both groups \n",
    "- The outcome data have **similar variances** across groups\n",
    "\n",
    "Now let's get a **sample of our data**, have a look at it, and check our assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26c3a8-3806-458c-b0c2-66257da9f847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "n <- 100\n",
    "P1_con_samp <- sample_n(df %>% filter(polysemy == 1) %>% dplyr::select(word, polysemy, concreteness) %>% na.omit(), n)\n",
    "P0_con_samp <- sample_n(df %>% filter(polysemy == 0) %>% dplyr::select(word, polysemy, concreteness) %>% na.omit(), n)\n",
    "\n",
    "P_con_samp <- rbind(P1_con_samp, P0_con_samp)\n",
    "\n",
    "ggplot(P_con_samp, aes(x=polysemy %>% as.factor(), y=concreteness)) + geom_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79958294-705d-4457-9d1a-0f791fcbdd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Are gender for nouns and gender for verbs approximately normally distributed? \n",
    "# test of normality: p-value <= 0.05 suggests data is NOT normally distributed\n",
    "print(shapiro.test(P1_con_samp$concreteness))\n",
    "print(shapiro.test(P0_con_samp$concreteness))\n",
    "\n",
    "# Do gender for nouns and gender for verbs have similar variances?\n",
    "# test of homogeneity of variances: p-value <= 0.05 suggests variances are equal\n",
    "print(var.test(concreteness ~ polysemy %>% as.factor(), data = P_con_samp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e8de4-bdc4-46cd-bb91-46f8ff4b5c5a",
   "metadata": {},
   "source": [
    "#### We should use the Mann-Whitney U test\n",
    "\n",
    "Since the assumptions are not satisfied, we choose the **Mann-Whitney U test** (non-parametric test).\n",
    "\n",
    "Now we can define our null and alternative hypotheses:\n",
    "- $H_{0}$: Concreteness for polysemous words **does not differ from** concreteness for non-polysemous words\n",
    "- $H_{A}$: Concreteness for polysemous **is greater than** concreteness for non-polysemous words\n",
    "\n",
    "Since we have a specific hypothesis about the direction of the relationship, this is a **one-sided** test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63e202-1516-43e6-9bb3-b2b24913992c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "MWU_test <- wilcox.test(P1_con_samp$concreteness,\n",
    "                       P0_con_samp$concreteness,\n",
    "                       alternative = \"greater\",\n",
    "                       paired=FALSE)\n",
    "MWU_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe331561-f2e2-416d-bbd8-a584854d3621",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The p-value is less than 0.05, which is our significance level, so we **reject the null hypothesis**. Therefore we conclude that **polysemous words are more concrete than non-polysemous words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d766be-89d9-4167-a67d-4981bb0b65b2",
   "metadata": {},
   "source": [
    "<a id='Chi'></a>\n",
    "### 3.C Is there a relationship between part of speech (noun/verb) and polysemy?\n",
    "\n",
    "#### What test should we use?\n",
    "\n",
    "**Step 1: Understand type of predictor and outcome variables.**\n",
    "\n",
    "- Predictor(s): 1 **Categorical** predictor (part of speech) with 2 groups (noun, verb)\n",
    "- Outcome(s): 1 **Categorical** outcome (polysemy) with 2 groups (polysemous, non-polysemous)\n",
    "\n",
    "#### We should use the Chi Square test of Independence\n",
    "\n",
    "For this type of data, we should use a **Chi Square test of Independence** since both the predictor and outcome variables are categorical. This is a non-parametric test. There is no parametric version of this test.\n",
    "\n",
    "Now we can define our null and alternative hypotheses:\n",
    "- $H_{0}$: Part of speech and polysemy are **independent**.\n",
    "- $H_{A}$: Part of speech and polysemy are **not independent**.\n",
    "\n",
    "**Step 2: Check the assumptions of the sampled data.**\n",
    "- Data must be frequencies in a contingency tables\n",
    "- Expected value of cells should be 5 or greater in at least 80% of cells\n",
    "\n",
    "If the assumption of the expected values of the cells being 5 or greater is not met, we should take a larger sample if possible.\n",
    "\n",
    "Now let's get a **sample of our data**, have a look at it, and check our assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23913e-9cc1-429f-bc16-4ea218f19bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "n  <- 200\n",
    "POS_poly_samp <- sample_n(df %>% filter(POS != 0) %>% dplyr::select(word, polysemy, POS) %>% na.omit(), n)\n",
    "POS_poly_samp <- POS_poly_samp %>% mutate_at(c('POS', 'polysemy'), as.factor)\n",
    "\n",
    "# make contingency table\n",
    "tab <- table(POS_poly_samp %>% dplyr::select(-word))\n",
    "\n",
    "# Chi Square test\n",
    "Chi_test <- chisq.test(tab)\n",
    "\n",
    "# Table of expected values\n",
    "# If at least 20% of cells are less than 5, take a bigger sample\n",
    "Chi_test$expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0143a1-aacc-4fde-9aea-be16948b0d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "Chi_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797876f-d229-4ab2-b842-4e7ca70228ad",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The p-value is greater than 0.05, which is our significance level, so we **fail to reject the null hypothesis**. Therefore we conclude that **there is no relationship between part of speech (noun/verb) and polysemy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18984e-0c75-4e8b-8faa-04c79c98d7fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='reg'></a>\n",
    "### 3.D Can imageability be used to predict word concreteness?\n",
    "\n",
    "#### What test should we use?\n",
    "\n",
    "**Step 1: Understand type of predictor and outcome variables.**\n",
    "\n",
    "- Predictor(s): 1 **Quantitative** predictor (imageability)\n",
    "- Outcome(s): 1 **Quantitative** outcome (concreteness)\n",
    "\n",
    "#### We should use linear regression\n",
    "\n",
    "To find a relationship between a quantitative predictor and a quantitative outcome, we could use a correlation analysis, but a correlation analysis does not assume any direction of the relationship. A regression analysis on the other hand **assumes a direction of the relationship**. In addition to quantifying the relationship between quantitative variables, regression models can be **used to make predictions** about outcome variable given new values of the predictor variable.\n",
    "\n",
    "**Simple linear regression vs. multiple linear regression**\n",
    "\n",
    "Simple linear regression involves only 1 predictor variable (and can be easily visualized in 2 dimensions), while multiple linear regression involves 2 or more predictor variables. The goal of regression is to find a **line of best fit** that best describes the relationship between the predictor(s) and outcome such that the **error is minimized**, typically, the **sum of squared error (SSE)** is minimized.\n",
    "\n",
    "In **simple linear regression**, the line of best fit is a function $Y = mX + b$ where $Y$ is the outcome (the variable we want to predict) and $X$ is the predictor. The goal is to find $m$ the slope and $b$ the intercept of the line to minimize the error. The coefficient $m$ represents the relationship between the predictor and the outcome. The **null hypothesis** is the the **coefficient is equal to zero**, reflecting no relationship between the predictor and the outcome.\n",
    "\n",
    "In **multiple linear regression** works in the same way as simple linear regression but the line of best fit is a function the line of best fit is a function $Y = m_1X_1 + m_2X_2 ... + m_nX_n + b$, so it has multiple predictors and coefficients to be estimated.\n",
    "\n",
    "**What output is produced by a regression model?** \n",
    "\n",
    "A regression model fit to data produces the following output:\n",
    "- **coefficient estimates** with corresponding p-values reflecting the relationship between predictor(s) and outcome\n",
    "- **r-squared** value reflecting the proportion of variance explained by the model\n",
    "- **f-statistic** with corresponding p-value reflecting how well the model fits the data overall\n",
    "\n",
    "Now we can define our null and alternative hypotheses:\n",
    "- $H_{0}$: There is no relationships between imageability and concreteness ($m$ = 0)\n",
    "- $H_{A}$: There is a positive relationship between imageability and concreteness ($m$ > 0)\n",
    "\n",
    "**Step 2: Check the assumptions of the sampled data.**\n",
    "- Linear relationship between predictor(s) and outcome\n",
    "- Residuals approximately normally distributed\n",
    "- Variance of residuals approximately uniformly distributed\n",
    "- Low correlation among predictors (low multicollinearity)\n",
    "\n",
    "The first assumption can be checked with a simple scatterplot. The second and third should be checked after the model is fit, since they are calculated on residuals, which is the difference between the actual outcomes and the predicted outcomes (falling on the line of best fit). The last assumption needs to be checked in the case of multiple linear regression in which there are multiple predictors.\n",
    "\n",
    "Now let's get a **sample of our data**, and view the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9352ff6-e4aa-4521-abff-320349bb8351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "set.seed(100)\n",
    "n  <- 200\n",
    "reg_samp <- sample_n(df_num, n)\n",
    "\n",
    "plot(concreteness ~ imageability, data = reg_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dffbe1-3f26-4d6c-9ef3-8ab683f3b7c2",
   "metadata": {},
   "source": [
    "There is clearly a **linear relationship** between the two variables.\n",
    "\n",
    "Now let's fit the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa3d39-2463-405a-941b-c53a0e5cc010",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "model <- lm(concreteness ~ imageability, data = reg_samp)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add0bad-35c4-4d7b-b371-b046dbc482c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# View the plot with the line of best fit\n",
    "plot(concreteness ~ imageability, data = reg_samp)\n",
    "abline(model, col = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ec731-4474-4e77-8fc5-a2203a1350ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "The models looks like a **good fit**, judging by the low p-value for the coefficient, the high R-squared, and the low p-value for the F-statistic, but we still **need to check the other assumptions**, since the estimates of the model depend on these assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d7b26-e6f8-4ead-8657-15a1990e9e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "par(mfrow=c(2,2))\n",
    "plot(model)\n",
    "par(mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08daad9c-8dc1-4397-bfd4-13e9f432f408",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The residuals appear to be approximately normally distributed and the variance of residuals appears to be approximately uniformly distributed.\n",
    "\n",
    "All of our assumptions are satisfied, and the model parameter estimates are significant, therefore we can conclude that **imageability can be used to predict word concreteness**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca032757-7e33-4fbb-8012-2b72ed8dc603",
   "metadata": {},
   "source": [
    "#### What if we do not know what variables are best for predicting a variable?\n",
    "\n",
    "In many cases, we do not know which combination of predictor variables is best for predicting an outcome.\n",
    "\n",
    "If we have too few predictors, we are not utilizing the full potential of our dataset to predict an outcome, but usually when it comes to regression models, less is more, because **too many predictor variables can lead to overfitting**. The best model maximizes **parsimony**, striking a balance between model simplicity and the ability of the model to explain the data accurately.\n",
    "\n",
    "The parsimony of a model can be measured with measures like:\n",
    "- **AIC** which rewards models for how well they explain the data while penalizing them for their complexity\n",
    "- **BIC** which imposes a stronger penalty for model complexity than AIC\n",
    "\n",
    "Considering two different models with different numbers of predictors, we can use the AIC and BIC to see which model is more parsimonious (lower values are better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1622bf-c195-4464-a25c-8bcd85233afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "model1 <- lm(concreteness ~ imageability + length, data = reg_samp)\n",
    "model2 <- lm(concreteness ~ imageability + length + aoa, data = reg_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f99198-8e5d-481e-b06a-e2b58f74bdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(AIC(model1))\n",
    "print(AIC(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8651c-a652-405b-924b-6ab296276601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(BIC(model1))\n",
    "print(BIC(model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8f48e-fdad-4118-b118-e6c0434b1982",
   "metadata": {},
   "source": [
    "According to the AIC the more complex model is better, while according to the BIC the simpler model is better. So the choice of AIC vs. BIC for assessing a model should be made considering what the model will be used for.\n",
    "\n",
    "There are also **methods for model selection** that find the best model (using AIC or BIC) from different combinations of predictors. They include:\n",
    "- **Forwards selection** which starts with no predictors in the model and adds predictors until the best model is reached\n",
    "- **Backwards selection** which starts with all predictors in the model and removes predictors until the best model is reached\n",
    "- **Stepwise selection** which is a combination of forward and backward selection\n",
    "\n",
    "Here we see an example of stepwise selection to select a model for predicting concreteness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2251a-eedc-4344-9c9e-f8eeda16016b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Fit the full model \n",
    "full.model <- lm(concreteness ~., data = reg_samp %>% dplyr::select(-polysemy, -POS))\n",
    "\n",
    "# Stepwise regression model\n",
    "step.model <- stepAIC(full.model, direction = \"both\", \n",
    "                      trace = FALSE)\n",
    "summary(step.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
